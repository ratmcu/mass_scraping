{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJvBjtzc76un"
   },
   "source": [
    "# **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "_UQatvRc7z6E",
    "outputId": "84dd13fb-b31d-446f-c741-c286343c6b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: boto3 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.206)\n",
      "Requirement already satisfied: tqdm in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.33.0)\n",
      "Requirement already satisfied: numpy in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.15.0)\n",
      "Requirement already satisfied: regex in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.6.8)\n",
      "Requirement already satisfied: requests in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.22.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.206 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.206)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.25.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.206->boto3->pytorch-pretrained-bert) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.15,>=0.10 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.206->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ratmcu/anaconda3/envs/pytorch/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.206->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "An entry or sent looks like ...\n",
    "SOCCER NN B-NP O\n",
    "- : O O\n",
    "JAPAN NNP B-NP B-LOC\n",
    "GET VB B-VP O\n",
    "LUCKY NNP B-NP O\n",
    "WIN NNP I-NP O\n",
    ", , O O\n",
    "CHINA NNP B-NP B-PER\n",
    "IN IN B-PP O\n",
    "SURPRISE DT B-NP O\n",
    "DEFEAT NN I-NP O\n",
    ". . O O\n",
    "Each mini-batch returns the followings:\n",
    "words: list of input sents. [\"The 26-year-old ...\", ...]\n",
    "x: encoded input sents. [N, T]. int64.\n",
    "is_heads: list of head markers. [[1, 1, 0, ...], [...]]\n",
    "tags: list of tags.['O O B-MISC ...', '...']\n",
    "y: encoded tags. [N, T]. int64\n",
    "seqlens: list of seqlens. [45, 49, 10, 50, ...]\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "!pip install pytorch-pretrained-bert\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "VOCAB = ('<PAD>', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG')\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
    "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n",
    "\n",
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, fpath):\n",
    "        \"\"\"\n",
    "        fpath: [train|valid|test].txt\n",
    "        \"\"\"\n",
    "        entries = open(fpath, 'r').read().strip().split(\"\\n\\n\")\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for entry in entries:\n",
    "            words = [line.split()[0] for line in entry.splitlines()]\n",
    "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
    "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "            tags_li.append([\"<PAD>\"] + tags + [\"<PAD>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<PAD>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "\n",
    "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen\n",
    "\n",
    "\n",
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujqrKA1e878N"
   },
   "source": [
    "# Testing Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XxxGFZj-uBF"
   },
   "source": [
    "download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "gUNNDnWm87VB",
    "outputId": "06abee43-9a0e-489a-830b-b3a13fc1eb8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘conll2003’: File exists\n",
      "--2019-08-16 22:06:09--  https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.124.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.124.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3283420 (3.1M) [text/plain]\n",
      "Saving to: ‘train.txt’\n",
      "\n",
      "train.txt           100%[===================>]   3.13M  8.50MB/s    in 0.4s    \n",
      "\n",
      "2019-08-16 22:06:09 (8.50 MB/s) - ‘train.txt’ saved [3283420/3283420]\n",
      "\n",
      "--2019-08-16 22:06:09--  https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/valid.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.124.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.124.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 827443 (808K) [text/plain]\n",
      "Saving to: ‘valid.txt’\n",
      "\n",
      "valid.txt           100%[===================>] 808.05K  14.2KB/s    in 57s     \n",
      "\n",
      "2019-08-16 22:07:07 (14.2 KB/s) - ‘valid.txt’ saved [827443/827443]\n",
      "\n",
      "--2019-08-16 22:07:07--  https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.124.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.124.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 748095 (731K) [text/plain]\n",
      "Saving to: ‘test.txt’\n",
      "\n",
      "test.txt            100%[===================>] 730.56K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2019-08-16 22:07:07 (9.07 MB/s) - ‘test.txt’ saved [748095/748095]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train='https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt'\n",
    "# valid='https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/valid.txt'\n",
    "# test='https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/test.txt'\n",
    "\n",
    "# !mkdir conll2003 | wget --show-progress $train && mv train.txt conll2003\n",
    "# !wget --show-progress $valid && mv valid.txt conll2003\n",
    "# !wget --show-progress $test && mv test.txt conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8hYm5Xn-yFR"
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--trainset\", type=str, default=\"conll2003/train.txt\")\n",
    "# parser.add_argument(\"--validset\", type=str, default=\"conll2003/valid.txt\")\n",
    "# hp = parser.parse_args()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model = Net(hp.top_rnns, len(VOCAB), device, hp.finetuning).cuda()\n",
    "# model = nn.DataParallel(model)\n",
    "\n",
    "train_dataset = NerDataset(\"conll2003/train.txt\")\n",
    "eval_dataset = NerDataset(\"conll2003/valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHm9k2UYG4VS"
   },
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1w-cG4bwGLfq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, top_rnns=False, vocab_size=None, device='cpu', finetuning=False):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.top_rnns=top_rnns\n",
    "        if top_rnns:\n",
    "            self.rnn = nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768//2, batch_first=True)\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.finetuning = finetuning\n",
    "\n",
    "    def forward(self, x, y, ):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        Returns\n",
    "        enc: (N, T, VOCAB)\n",
    "        '''\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        if self.training and self.finetuning:\n",
    "            # print(\"->bert.train()\")\n",
    "            self.bert.train()\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "\n",
    "        if self.top_rnns:\n",
    "            enc, _ = self.rnn(enc)\n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjF9fet1G7N9"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "yUgSDE_AJqRb",
    "outputId": "9b1f11fd-a0d5-4ec0-a4eb-f7a237f8fb6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "    batch_size = 16\n",
      "    lr = 0.0001\n",
      "    n_epochs = 30\n",
      "    finetuning = True\n",
      "    top_rnns = False\n",
      "433796608\n",
      "937\n",
      "=====sanity check======\n",
      "words: [CLS] DUBAI 1996-08-22 [SEP]\n",
      "x: [ 101  141 2591 8215 2240 1820  118 4775  118 1659  102]\n",
      "tokens: ['[CLS]', 'D', '##U', '##BA', '##I', '1996', '-', '08', '-', '22', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "y: [0 8 0 0 0 1 0 0 0 0 0]\n",
      "tags: <PAD> B-LOC O <PAD>\n",
      "seqlen: 11\n",
      "=======================\n",
      "step: 0, loss: 2.2521984577178955\n",
      "step: 10, loss: 0.436301052570343\n",
      "step: 20, loss: 0.3271217346191406\n",
      "step: 30, loss: 0.09676904231309891\n",
      "step: 40, loss: 0.10202603042125702\n",
      "step: 50, loss: 0.23763076961040497\n",
      "step: 60, loss: 0.23879751563072205\n",
      "step: 70, loss: 0.15176692605018616\n",
      "step: 80, loss: 0.06664877384901047\n",
      "step: 90, loss: 0.038849838078022\n",
      "step: 100, loss: 0.1720307469367981\n",
      "step: 110, loss: 0.1923402100801468\n",
      "step: 120, loss: 0.09545484930276871\n",
      "step: 130, loss: 0.08768244087696075\n",
      "step: 140, loss: 0.24690145254135132\n",
      "step: 150, loss: 0.11757929623126984\n",
      "step: 160, loss: 0.03903689980506897\n",
      "step: 170, loss: 0.35160765051841736\n",
      "step: 180, loss: 0.028361260890960693\n",
      "step: 190, loss: 0.05634159967303276\n",
      "step: 200, loss: 0.08320371061563492\n",
      "step: 210, loss: 0.040100645273923874\n",
      "step: 220, loss: 0.2044624537229538\n",
      "step: 230, loss: 0.12861214578151703\n",
      "step: 240, loss: 0.06679131835699081\n",
      "step: 250, loss: 0.10163462907075882\n",
      "step: 260, loss: 0.06759776920080185\n",
      "step: 270, loss: 0.13603654503822327\n",
      "step: 280, loss: 0.05635548010468483\n",
      "step: 290, loss: 0.1702801138162613\n",
      "step: 300, loss: 0.09692980349063873\n",
      "step: 310, loss: 0.03524108976125717\n",
      "step: 320, loss: 0.0034126117825508118\n",
      "step: 330, loss: 0.060424286872148514\n",
      "step: 340, loss: 0.06942388415336609\n",
      "step: 350, loss: 0.07173316180706024\n",
      "step: 360, loss: 0.10075242817401886\n",
      "step: 370, loss: 0.06098154932260513\n",
      "step: 380, loss: 0.06308895349502563\n",
      "step: 390, loss: 0.10790663957595825\n",
      "step: 400, loss: 0.05920807272195816\n",
      "step: 410, loss: 0.12465590238571167\n",
      "step: 420, loss: 0.060240600258111954\n",
      "step: 430, loss: 0.09293919056653976\n",
      "step: 440, loss: 0.030951643362641335\n",
      "step: 450, loss: 0.03357994183897972\n",
      "step: 460, loss: 0.06000238656997681\n",
      "step: 470, loss: 0.019605958834290504\n",
      "step: 480, loss: 0.04955758899450302\n",
      "step: 490, loss: 0.15119217336177826\n",
      "step: 500, loss: 0.05217765271663666\n",
      "step: 510, loss: 0.0903421938419342\n",
      "step: 520, loss: 0.0390925295650959\n",
      "step: 530, loss: 0.017215043306350708\n",
      "step: 540, loss: 0.035144172608852386\n",
      "step: 550, loss: 0.06649529933929443\n",
      "step: 560, loss: 0.05413421615958214\n",
      "step: 570, loss: 0.05666561797261238\n",
      "step: 580, loss: 0.10165160149335861\n",
      "step: 590, loss: 0.08617480099201202\n",
      "step: 600, loss: 0.04744601622223854\n",
      "step: 610, loss: 0.07048877328634262\n",
      "step: 620, loss: 0.017130518332123756\n",
      "step: 630, loss: 0.013992591761052608\n",
      "step: 640, loss: 0.04541027545928955\n",
      "step: 650, loss: 0.05082238093018532\n",
      "step: 660, loss: 0.037018515169620514\n",
      "step: 670, loss: 0.03709661588072777\n",
      "=========eval at epoch=1=========\n",
      "num_proposed:8535\n",
      "num_correct:8001\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/1.pt\n",
      "=====sanity check======\n",
      "words: [CLS] 1-0 . [SEP]\n",
      "x: [101 122 118 121 119 102]\n",
      "tokens: ['[CLS]', '1', '-', '0', '.', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 1, 1]\n",
      "y: [0 1 0 0 1 0]\n",
      "tags: <PAD> O O <PAD>\n",
      "seqlen: 6\n",
      "=======================\n",
      "step: 0, loss: 0.07283417135477066\n",
      "step: 10, loss: 0.07932772487401962\n",
      "step: 20, loss: 0.032768841832876205\n",
      "step: 30, loss: 0.010220401920378208\n",
      "step: 40, loss: 0.005809349939227104\n",
      "step: 50, loss: 0.08570899814367294\n",
      "step: 60, loss: 0.0132832620292902\n",
      "=========eval at epoch=2=========\n",
      "num_proposed:8459\n",
      "num_correct:7907\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.92\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/2.pt\n",
      "=====sanity check======\n",
      "words: [CLS] CHICAGO AT ATLANTA [SEP]\n",
      "x: [  101 24890  9741 22689  2346 13020 13020 10783 15681  1592   102]\n",
      "tokens: ['[CLS]', 'CH', '##IC', '##AG', '##O', 'AT', 'AT', '##LA', '##NT', '##A', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "y: [0 9 0 0 0 1 8 0 0 0 0]\n",
      "tags: <PAD> B-ORG O B-LOC <PAD>\n",
      "seqlen: 11\n",
      "=======================\n",
      "step: 0, loss: 0.07664420455694199\n",
      "step: 10, loss: 0.02433185838162899\n",
      "step: 20, loss: 0.04061412066221237\n",
      "step: 30, loss: 0.009015761315822601\n",
      "step: 40, loss: 0.09249874949455261\n",
      "step: 50, loss: 0.009381153620779514\n",
      "step: 60, loss: 0.017536811530590057\n",
      "step: 70, loss: 0.023208022117614746\n",
      "step: 80, loss: 0.08465970307588577\n",
      "step: 90, loss: 0.012613626196980476\n",
      "step: 100, loss: 0.13499173521995544\n",
      "step: 110, loss: 0.09673887491226196\n",
      "step: 120, loss: 0.02933136373758316\n",
      "step: 130, loss: 0.011650258675217628\n",
      "step: 140, loss: 0.055420663207769394\n",
      "step: 150, loss: 0.07758546620607376\n",
      "step: 160, loss: 0.1264977753162384\n",
      "step: 170, loss: 0.07940413057804108\n",
      "step: 180, loss: 0.10844185203313828\n",
      "step: 190, loss: 0.09629059582948685\n",
      "step: 200, loss: 0.0160830058157444\n",
      "step: 210, loss: 0.07813286036252975\n",
      "step: 220, loss: 0.027033699676394463\n",
      "step: 230, loss: 0.036755166947841644\n",
      "step: 240, loss: 0.06107105687260628\n",
      "step: 250, loss: 0.08551471680402756\n",
      "step: 260, loss: 0.055370163172483444\n",
      "step: 270, loss: 0.03663436323404312\n",
      "step: 280, loss: 0.028972648084163666\n",
      "step: 290, loss: 0.05814044177532196\n",
      "step: 300, loss: 0.04370458424091339\n",
      "step: 310, loss: 0.014993385411798954\n",
      "step: 320, loss: 0.09318975359201431\n",
      "step: 330, loss: 0.006271643098443747\n",
      "step: 340, loss: 0.042336225509643555\n",
      "step: 350, loss: 0.06560255587100983\n",
      "step: 360, loss: 0.02854684554040432\n",
      "step: 370, loss: 0.03909183293581009\n",
      "step: 380, loss: 0.0753377303481102\n",
      "step: 390, loss: 0.06114765256643295\n",
      "step: 400, loss: 0.01173607911914587\n",
      "step: 410, loss: 0.004539409186691046\n",
      "step: 420, loss: 0.010100722312927246\n",
      "step: 430, loss: 0.022446468472480774\n",
      "step: 440, loss: 0.02157757431268692\n",
      "step: 450, loss: 0.022999413311481476\n",
      "step: 460, loss: 0.11639039218425751\n",
      "step: 470, loss: 0.0972968339920044\n",
      "step: 480, loss: 0.01301674172282219\n",
      "step: 490, loss: 0.08971887081861496\n",
      "step: 500, loss: 0.018692608922719955\n",
      "step: 510, loss: 0.0401020422577858\n",
      "step: 520, loss: 0.04186041280627251\n",
      "step: 530, loss: 0.022952403873205185\n",
      "step: 540, loss: 0.030190931633114815\n",
      "step: 550, loss: 0.026393016800284386\n",
      "step: 560, loss: 0.03960106521844864\n",
      "step: 570, loss: 0.0571616105735302\n",
      "step: 580, loss: 0.03425685316324234\n",
      "step: 590, loss: 0.0023058552760630846\n",
      "step: 600, loss: 0.040640536695718765\n",
      "step: 610, loss: 0.08456621319055557\n",
      "step: 620, loss: 0.02672753296792507\n",
      "step: 630, loss: 0.02578437328338623\n",
      "=========eval at epoch=3=========\n",
      "num_proposed:8612\n",
      "num_correct:7963\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/3.pt\n",
      "=====sanity check======\n",
      "words: [CLS] Six months to June 30 [SEP]\n",
      "x: [ 101 4995 1808 1106 1340 1476  102]\n",
      "tokens: ['[CLS]', 'Six', 'months', 'to', 'June', '30', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O <PAD>\n",
      "seqlen: 7\n",
      "=======================\n",
      "step: 0, loss: 0.004343857988715172\n",
      "step: 10, loss: 0.003868348663672805\n",
      "step: 20, loss: 0.009994694963097572\n",
      "step: 30, loss: 0.030867861583828926\n",
      "step: 40, loss: 0.023562615737318993\n",
      "step: 50, loss: 0.009695319458842278\n",
      "step: 60, loss: 0.05065348744392395\n",
      "step: 70, loss: 0.14378699660301208\n",
      "step: 80, loss: 0.014371438883244991\n",
      "step: 90, loss: 0.022831453010439873\n",
      "step: 100, loss: 0.01668156497180462\n",
      "step: 110, loss: 0.02826862968504429\n",
      "step: 120, loss: 0.020019259303808212\n",
      "step: 130, loss: 0.02916591800749302\n",
      "step: 140, loss: 0.012742764316499233\n",
      "step: 150, loss: 0.030786897987127304\n",
      "step: 160, loss: 0.07733693718910217\n",
      "step: 170, loss: 0.008461970835924149\n",
      "step: 180, loss: 0.021670952439308167\n",
      "step: 190, loss: 0.012165379710495472\n",
      "step: 200, loss: 0.049249738454818726\n",
      "step: 210, loss: 0.018602993339300156\n",
      "step: 220, loss: 0.005752937868237495\n",
      "step: 230, loss: 0.01351621188223362\n",
      "step: 240, loss: 0.010858315043151379\n",
      "step: 250, loss: 0.016203435137867928\n",
      "step: 260, loss: 0.0012034995015710592\n",
      "step: 270, loss: 0.005532931070774794\n",
      "step: 280, loss: 0.06593959778547287\n",
      "step: 290, loss: 0.019725650548934937\n",
      "step: 300, loss: 0.015038830228149891\n",
      "step: 310, loss: 0.025589676573872566\n",
      "step: 320, loss: 0.06127326935529709\n",
      "step: 330, loss: 0.019010039046406746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 340, loss: 0.0661143958568573\n",
      "step: 350, loss: 0.00867249071598053\n",
      "step: 360, loss: 0.006173949223011732\n",
      "step: 370, loss: 0.05764853209257126\n",
      "step: 380, loss: 0.02455427683889866\n",
      "step: 390, loss: 0.06349090486764908\n",
      "step: 400, loss: 0.06947939842939377\n",
      "step: 410, loss: 0.03597579151391983\n",
      "step: 420, loss: 0.04264821857213974\n",
      "step: 430, loss: 0.02358974888920784\n",
      "step: 440, loss: 0.003530247835442424\n",
      "step: 450, loss: 0.055052921175956726\n",
      "step: 460, loss: 0.04485927149653435\n",
      "step: 470, loss: 0.05754230543971062\n",
      "step: 480, loss: 0.033246997743844986\n",
      "step: 490, loss: 0.006853921338915825\n",
      "step: 500, loss: 0.026338182389736176\n",
      "step: 510, loss: 0.11784500628709793\n",
      "step: 520, loss: 0.018559681251645088\n",
      "step: 530, loss: 0.04403621703386307\n",
      "step: 540, loss: 0.00773985730484128\n",
      "step: 550, loss: 0.06577816605567932\n",
      "step: 560, loss: 0.029047703370451927\n",
      "step: 570, loss: 0.006913188844919205\n",
      "step: 580, loss: 0.03171537071466446\n",
      "step: 590, loss: 0.014535246416926384\n",
      "step: 600, loss: 0.01853082701563835\n",
      "step: 610, loss: 0.009255596436560154\n",
      "step: 620, loss: 0.030661430209875107\n",
      "step: 630, loss: 0.022834526374936104\n",
      "step: 640, loss: 0.025476213544607162\n",
      "step: 650, loss: 0.019224414601922035\n",
      "step: 660, loss: 0.0730438083410263\n",
      "step: 670, loss: 0.07867060601711273\n",
      "step: 680, loss: 0.13556309044361115\n",
      "step: 690, loss: 0.08874156326055527\n",
      "=========eval at epoch=4=========\n",
      "num_proposed:8595\n",
      "num_correct:7898\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/4.pt\n",
      "=====sanity check======\n",
      "words: [CLS] -DOCSTART- [SEP]\n",
      "x: [  101   118   141  9244  9272 12426  1942   118   102]\n",
      "tokens: ['[CLS]', '-', 'D', '##OC', '##ST', '##AR', '##T', '-', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "y: [0 1 0 0 0 0 0 0 0]\n",
      "tags: <PAD> O <PAD>\n",
      "seqlen: 9\n",
      "=======================\n",
      "step: 0, loss: 0.07812121510505676\n",
      "step: 10, loss: 0.05595502629876137\n",
      "step: 20, loss: 0.033003535121679306\n",
      "step: 30, loss: 0.1204519048333168\n",
      "step: 40, loss: 0.020520083606243134\n",
      "step: 50, loss: 0.010926248505711555\n",
      "step: 60, loss: 0.03103875368833542\n",
      "step: 70, loss: 0.025647293776273727\n",
      "step: 80, loss: 0.0028117469046264887\n",
      "step: 90, loss: 0.06361506134271622\n",
      "step: 100, loss: 0.025964785367250443\n",
      "step: 110, loss: 0.019245395436882973\n",
      "step: 120, loss: 0.009466437622904778\n",
      "step: 130, loss: 0.025849832221865654\n",
      "step: 140, loss: 0.10088052600622177\n",
      "step: 150, loss: 0.003492331597954035\n",
      "step: 160, loss: 0.0052328589372336864\n",
      "step: 170, loss: 0.01487087644636631\n",
      "step: 180, loss: 0.01893095299601555\n",
      "step: 190, loss: 0.0034351730719208717\n",
      "step: 200, loss: 0.009530527517199516\n",
      "step: 210, loss: 0.010201399214565754\n",
      "step: 220, loss: 0.009714634157717228\n",
      "step: 230, loss: 0.0009505576454102993\n",
      "step: 240, loss: 0.007793926168233156\n",
      "step: 250, loss: 0.0019730613566935062\n",
      "step: 260, loss: 0.07377884536981583\n",
      "step: 270, loss: 0.020566551014780998\n",
      "step: 280, loss: 0.024372875690460205\n",
      "step: 290, loss: 0.06412573158740997\n",
      "step: 300, loss: 0.11733740568161011\n",
      "step: 310, loss: 0.1002158671617508\n",
      "step: 320, loss: 0.01618855632841587\n",
      "step: 330, loss: 0.010191002860665321\n",
      "step: 340, loss: 0.02119932882487774\n",
      "=========eval at epoch=5=========\n",
      "num_proposed:8587\n",
      "num_correct:7984\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/5.pt\n",
      "=====sanity check======\n",
      "words: [CLS] JERUSALEM 1996-08-22 [SEP]\n",
      "x: [  101   147  9637 13329 12507 15577  1820   118  4775   118  1659   102]\n",
      "tokens: ['[CLS]', 'J', '##ER', '##US', '##AL', '##EM', '1996', '-', '08', '-', '22', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "y: [0 8 0 0 0 0 1 0 0 0 0 0]\n",
      "tags: <PAD> B-LOC O <PAD>\n",
      "seqlen: 12\n",
      "=======================\n",
      "step: 0, loss: 0.0022583610843867064\n",
      "step: 10, loss: 0.0008115705568343401\n",
      "step: 20, loss: 0.018495241180062294\n",
      "step: 30, loss: 0.0013544989051297307\n",
      "step: 40, loss: 0.018367987126111984\n",
      "step: 50, loss: 0.0048493072390556335\n",
      "step: 60, loss: 0.005026849918067455\n",
      "step: 70, loss: 0.04399435222148895\n",
      "step: 80, loss: 0.0467192716896534\n",
      "step: 90, loss: 0.044295940548181534\n",
      "step: 100, loss: 0.01250650268048048\n",
      "step: 110, loss: 0.02076859213411808\n",
      "step: 120, loss: 0.02474023960530758\n",
      "step: 130, loss: 0.018683960661292076\n",
      "step: 140, loss: 0.012268127873539925\n",
      "step: 150, loss: 0.021428542211651802\n",
      "step: 160, loss: 0.002666883636265993\n",
      "step: 170, loss: 0.021314352750778198\n",
      "step: 180, loss: 0.027092503383755684\n",
      "step: 190, loss: 0.03263420984148979\n",
      "step: 200, loss: 0.0018091823440045118\n",
      "step: 210, loss: 0.004613442346453667\n",
      "=========eval at epoch=6=========\n",
      "num_proposed:8557\n",
      "num_correct:8013\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/6.pt\n",
      "=====sanity check======\n",
      "words: [CLS] 2. Tom Pukstys ( U.S. ) 84.20 [SEP]\n",
      "x: [  101   123   119  2545   153  7563 13913  1116   113   158   119   156\n",
      "   119   114  5731   119  1406   102]\n",
      "tokens: ['[CLS]', '2', '.', 'Tom', 'P', '##uk', '##sty', '##s', '(', 'U', '.', 'S', '.', ')', '84', '.', '20', '[SEP]']\n",
      "is_heads: [1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "y: [0 1 0 3 4 0 0 0 1 8 0 0 0 1 1 0 0 0]\n",
      "tags: <PAD> O B-PER I-PER O B-LOC O O <PAD>\n",
      "seqlen: 18\n",
      "=======================\n",
      "step: 0, loss: 0.0039454493671655655\n",
      "step: 10, loss: 0.0007596976938657463\n",
      "step: 20, loss: 0.011289249174296856\n",
      "step: 30, loss: 0.003542273538187146\n",
      "step: 40, loss: 0.009019989520311356\n",
      "step: 50, loss: 0.022861432284116745\n",
      "step: 60, loss: 0.013605636544525623\n",
      "step: 70, loss: 0.15398412942886353\n",
      "step: 80, loss: 0.0663902685046196\n",
      "step: 90, loss: 0.018050095066428185\n",
      "step: 100, loss: 0.03593399375677109\n",
      "step: 110, loss: 0.01258766371756792\n",
      "step: 120, loss: 0.053250908851623535\n",
      "step: 130, loss: 0.0076599521562457085\n",
      "step: 140, loss: 0.002634064294397831\n",
      "step: 150, loss: 0.035649511963129044\n",
      "step: 160, loss: 0.016652235761284828\n",
      "step: 170, loss: 0.07638110965490341\n",
      "step: 180, loss: 0.0070472112856805325\n",
      "step: 190, loss: 0.006138334982097149\n",
      "step: 200, loss: 0.042178891599178314\n",
      "step: 210, loss: 0.04870672523975372\n",
      "step: 220, loss: 0.05586148798465729\n",
      "step: 230, loss: 0.047424688935279846\n",
      "step: 240, loss: 0.010028761811554432\n",
      "step: 250, loss: 0.013252503238618374\n",
      "step: 260, loss: 0.004366760607808828\n",
      "step: 270, loss: 0.1709703654050827\n",
      "step: 280, loss: 0.1308521181344986\n",
      "step: 290, loss: 0.01449497975409031\n",
      "step: 300, loss: 0.024888643994927406\n",
      "step: 310, loss: 0.006171338725835085\n",
      "step: 320, loss: 0.006952490657567978\n",
      "=========eval at epoch=7=========\n",
      "num_proposed:8607\n",
      "num_correct:8032\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/7.pt\n",
      "=====sanity check======\n",
      "words: [CLS] Tromso 2 Kongsvinger 1 [SEP]\n",
      "x: [  101   157 16071  7301   123  3462  1116  3970  1200   122   102]\n",
      "tokens: ['[CLS]', 'T', '##rom', '##so', '2', 'Kong', '##s', '##ving', '##er', '1', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
      "y: [0 9 0 0 1 9 0 0 0 1 0]\n",
      "tags: <PAD> B-ORG O B-ORG O <PAD>\n",
      "seqlen: 11\n",
      "=======================\n",
      "step: 0, loss: 0.007818379439413548\n",
      "step: 10, loss: 0.03820095956325531\n",
      "step: 20, loss: 0.07213681936264038\n",
      "step: 30, loss: 0.0069359843619167805\n",
      "step: 40, loss: 0.0013181610265746713\n",
      "step: 50, loss: 0.005172121804207563\n",
      "step: 60, loss: 0.03236839175224304\n",
      "step: 70, loss: 0.011496842838823795\n",
      "step: 80, loss: 0.02048342488706112\n",
      "step: 90, loss: 0.031276363879442215\n",
      "step: 100, loss: 0.014667105861008167\n",
      "step: 110, loss: 0.027457617223262787\n",
      "step: 120, loss: 0.0011804410023614764\n",
      "step: 130, loss: 0.0013654730282723904\n",
      "step: 140, loss: 0.03920638933777809\n",
      "step: 150, loss: 0.02066640742123127\n",
      "step: 160, loss: 0.0015525870257988572\n",
      "step: 170, loss: 0.0025550350546836853\n",
      "step: 180, loss: 0.04702485725283623\n",
      "step: 190, loss: 0.03925417736172676\n",
      "step: 200, loss: 0.006497269030660391\n",
      "step: 210, loss: 0.0015335396165028214\n",
      "step: 220, loss: 0.0015959653537720442\n",
      "step: 230, loss: 0.005703379400074482\n",
      "step: 240, loss: 0.0013454532017931342\n",
      "step: 250, loss: 0.01803506724536419\n",
      "step: 260, loss: 0.017400434240698814\n",
      "step: 270, loss: 0.0022289417684078217\n",
      "step: 280, loss: 0.0007471679709851742\n",
      "step: 290, loss: 0.0037945962976664305\n",
      "step: 300, loss: 0.02017933689057827\n",
      "step: 310, loss: 0.045026473701000214\n",
      "step: 320, loss: 0.008010481484234333\n",
      "step: 330, loss: 0.01996654085814953\n",
      "step: 340, loss: 0.03158468380570412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 350, loss: 0.008036043494939804\n",
      "step: 360, loss: 0.005333912093192339\n",
      "step: 370, loss: 0.058056894689798355\n",
      "step: 380, loss: 0.03121345303952694\n",
      "step: 390, loss: 0.002361691789701581\n",
      "step: 400, loss: 0.06225937232375145\n",
      "step: 410, loss: 0.01968599669635296\n",
      "step: 420, loss: 0.0015107366489246488\n",
      "step: 430, loss: 0.03909780830144882\n",
      "step: 440, loss: 0.008962721563875675\n",
      "step: 450, loss: 0.01752508245408535\n",
      "step: 460, loss: 0.0007687830366194248\n",
      "step: 470, loss: 0.01131421234458685\n",
      "step: 480, loss: 0.03618811070919037\n",
      "step: 490, loss: 0.011290294118225574\n",
      "step: 500, loss: 0.07933424413204193\n",
      "step: 510, loss: 0.0502014085650444\n",
      "step: 520, loss: 0.006985587999224663\n",
      "step: 530, loss: 0.0060341269709169865\n",
      "step: 540, loss: 0.008227941580116749\n",
      "step: 550, loss: 0.0008974510128609836\n",
      "step: 560, loss: 0.010326836258172989\n",
      "step: 570, loss: 0.10052009671926498\n",
      "step: 580, loss: 0.059629887342453\n",
      "step: 590, loss: 0.02008405700325966\n",
      "step: 600, loss: 0.010087025351822376\n",
      "step: 610, loss: 0.003441210836172104\n",
      "step: 620, loss: 0.013685435056686401\n",
      "step: 630, loss: 0.026440758258104324\n",
      "step: 640, loss: 0.0017932777991518378\n",
      "step: 650, loss: 0.031321000307798386\n",
      "step: 660, loss: 0.0654764249920845\n",
      "step: 670, loss: 0.009847193956375122\n",
      "step: 680, loss: 0.06859888136386871\n",
      "step: 690, loss: 0.015407351776957512\n",
      "step: 700, loss: 0.006573574151843786\n",
      "step: 710, loss: 0.0316779688000679\n",
      "step: 720, loss: 0.0298015009611845\n",
      "step: 730, loss: 0.016985133290290833\n",
      "step: 740, loss: 0.024786105379462242\n",
      "=========eval at epoch=8=========\n",
      "num_proposed:8572\n",
      "num_correct:7993\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/8.pt\n",
      "=====sanity check======\n",
      "words: [CLS] In the future , the accountant will be evaluated solely on \" knowledge , skill and abilities , \" she said . [SEP]\n",
      "x: [  101  1130  1103  2174   117  1103 23195  1209  1129 17428  9308  1113\n",
      "   107  3044   117  7864  1105  7134   117   107  1131  1163   119   102]\n",
      "tokens: ['[CLS]', 'In', 'the', 'future', ',', 'the', 'accountant', 'will', 'be', 'evaluated', 'solely', 'on', '\"', 'knowledge', ',', 'skill', 'and', 'abilities', ',', '\"', 'she', 'said', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O O O O O O O O O O O O O O O <PAD>\n",
      "seqlen: 24\n",
      "=======================\n",
      "step: 0, loss: 0.01162163820117712\n",
      "step: 10, loss: 0.009548250585794449\n",
      "step: 20, loss: 0.0301573034375906\n",
      "step: 30, loss: 0.016572153195738792\n",
      "step: 40, loss: 0.007426771800965071\n",
      "step: 50, loss: 0.002671911148354411\n",
      "step: 60, loss: 0.012978943064808846\n",
      "step: 70, loss: 0.015702808275818825\n",
      "step: 80, loss: 0.049860645085573196\n",
      "step: 90, loss: 0.011676955968141556\n",
      "step: 100, loss: 0.0717398151755333\n",
      "step: 110, loss: 0.009792677126824856\n",
      "step: 120, loss: 0.031844813376665115\n",
      "step: 130, loss: 0.014304674230515957\n",
      "step: 140, loss: 0.11780054122209549\n",
      "step: 150, loss: 0.04312640428543091\n",
      "step: 160, loss: 0.08916020393371582\n",
      "step: 170, loss: 0.01898578181862831\n",
      "step: 180, loss: 0.0016800727462396026\n",
      "step: 190, loss: 0.02574487030506134\n",
      "step: 200, loss: 0.01568518579006195\n",
      "step: 210, loss: 0.024993177503347397\n",
      "step: 220, loss: 0.010939748957753181\n",
      "step: 230, loss: 0.004447319079190493\n",
      "step: 240, loss: 0.02795085683465004\n",
      "step: 250, loss: 0.004420130047947168\n",
      "step: 260, loss: 0.045280639082193375\n",
      "step: 270, loss: 0.009171390905976295\n",
      "step: 280, loss: 0.0006700243102386594\n",
      "step: 290, loss: 0.03407878056168556\n",
      "step: 300, loss: 0.002345912391319871\n",
      "step: 310, loss: 0.0010103527456521988\n",
      "step: 320, loss: 0.0005363254458643496\n",
      "step: 330, loss: 0.00644258689135313\n",
      "step: 340, loss: 0.013521527871489525\n",
      "step: 350, loss: 0.011259771883487701\n",
      "step: 360, loss: 0.014547083526849747\n",
      "step: 370, loss: 0.015144671313464642\n",
      "step: 380, loss: 0.00463307648897171\n",
      "step: 390, loss: 0.015139363706111908\n",
      "step: 400, loss: 0.006222694180905819\n",
      "step: 410, loss: 0.05118227005004883\n",
      "step: 420, loss: 0.010018283501267433\n",
      "step: 430, loss: 0.023846019059419632\n",
      "step: 440, loss: 0.0030722415540367365\n",
      "step: 450, loss: 0.008797743357717991\n",
      "step: 460, loss: 0.00891021266579628\n",
      "step: 470, loss: 0.011233351193368435\n",
      "step: 480, loss: 0.03940105810761452\n",
      "step: 490, loss: 0.06433483213186264\n",
      "step: 500, loss: 0.0039137895219028\n",
      "step: 510, loss: 0.003473229706287384\n",
      "step: 520, loss: 0.002942324848845601\n",
      "step: 530, loss: 0.028947651386260986\n",
      "step: 540, loss: 0.0011175338877364993\n",
      "step: 550, loss: 0.036879125982522964\n",
      "step: 560, loss: 0.020175345242023468\n",
      "step: 570, loss: 0.0041900863870978355\n",
      "step: 580, loss: 0.035408154129981995\n",
      "step: 590, loss: 0.05844641104340553\n",
      "=========eval at epoch=9=========\n",
      "num_proposed:8450\n",
      "num_correct:7939\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.92\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/9.pt\n",
      "=====sanity check======\n",
      "words: [CLS] SEATTLE AT BOSTON [SEP]\n",
      "x: [  101 12342 13821 20156  2036 13020   139  9025 18082  2249   102]\n",
      "tokens: ['[CLS]', 'SE', '##AT', '##TL', '##E', 'AT', 'B', '##OS', '##TO', '##N', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "y: [0 9 0 0 0 1 8 0 0 0 0]\n",
      "tags: <PAD> B-ORG O B-LOC <PAD>\n",
      "seqlen: 11\n",
      "=======================\n",
      "step: 0, loss: 0.043624598532915115\n",
      "step: 10, loss: 0.017389357089996338\n",
      "step: 20, loss: 0.0008125068852677941\n",
      "step: 30, loss: 0.0040204934775829315\n",
      "step: 40, loss: 0.007324185222387314\n",
      "step: 50, loss: 0.022018983960151672\n",
      "step: 60, loss: 0.001468931557610631\n",
      "step: 70, loss: 0.0367470420897007\n",
      "step: 80, loss: 0.03245076909661293\n",
      "step: 90, loss: 0.04642138257622719\n",
      "step: 100, loss: 0.00626504048705101\n",
      "step: 110, loss: 0.007301338482648134\n",
      "step: 120, loss: 0.0003958042652811855\n",
      "step: 130, loss: 0.0016926369862630963\n",
      "step: 140, loss: 0.0015041850274428725\n",
      "=========eval at epoch=10=========\n",
      "num_proposed:8572\n",
      "num_correct:7988\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/10.pt\n",
      "=====sanity check======\n",
      "words: [CLS] The survey , conducted in late 1995 and the early part of this year by management consulting firm Towers Perrin , showed that the focus will be on an employee 's overall value to the company 's bottom line -- rather than how well an employee performs a specific task . [SEP]\n",
      "x: [  101  1109  5980   117  3303  1107  1523  1876  1105  1103  1346  1226\n",
      "  1104  1142  1214  1118  2635 12421  3016 17879 14286  4854   117  2799\n",
      "  1115  1103  2817  1209  1129  1113  1126  7775   112   188  2905  2860\n",
      "  1106  1103  1419   112   188  3248  1413   118   118  1897  1190  1293\n",
      "  1218  1126  7775 10383   170  2747  4579   119   102]\n",
      "tokens: ['[CLS]', 'The', 'survey', ',', 'conducted', 'in', 'late', '1995', 'and', 'the', 'early', 'part', 'of', 'this', 'year', 'by', 'management', 'consulting', 'firm', 'Towers', 'Per', '##rin', ',', 'showed', 'that', 'the', 'focus', 'will', 'be', 'on', 'an', 'employee', \"'\", 's', 'overall', 'value', 'to', 'the', 'company', \"'\", 's', 'bottom', 'line', '-', '-', 'rather', 'than', 'how', 'well', 'an', 'employee', 'performs', 'a', 'specific', 'task', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 9 5 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O <PAD>\n",
      "seqlen: 57\n",
      "=======================\n",
      "step: 0, loss: 0.0019342205487191677\n",
      "step: 10, loss: 0.031804487109184265\n",
      "step: 20, loss: 0.006147326901555061\n",
      "step: 30, loss: 0.0007546562701463699\n",
      "step: 40, loss: 0.1521722972393036\n",
      "step: 50, loss: 0.004267030395567417\n",
      "step: 60, loss: 0.001963540446013212\n",
      "step: 70, loss: 0.023372402414679527\n",
      "step: 80, loss: 0.0021233465522527695\n",
      "step: 90, loss: 0.001309480401687324\n",
      "step: 100, loss: 0.0014178601559251547\n",
      "step: 110, loss: 0.00045109185157343745\n",
      "step: 120, loss: 0.001643592375330627\n",
      "step: 130, loss: 0.0006329270545393229\n",
      "step: 140, loss: 0.022785786539316177\n",
      "step: 150, loss: 0.013278594240546227\n",
      "step: 160, loss: 0.016690367832779884\n",
      "step: 170, loss: 0.009921391494572163\n",
      "step: 180, loss: 0.06318079680204391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 190, loss: 0.041838422417640686\n",
      "step: 200, loss: 0.0033136452548205853\n",
      "step: 210, loss: 0.0026937646325677633\n",
      "step: 220, loss: 0.0010001322953030467\n",
      "step: 230, loss: 0.01746082492172718\n",
      "step: 240, loss: 0.007711085025221109\n",
      "step: 250, loss: 0.045195117592811584\n",
      "step: 260, loss: 0.008755709044635296\n",
      "step: 270, loss: 0.002219368238002062\n",
      "step: 280, loss: 0.0006498199072666466\n",
      "step: 290, loss: 0.009938313625752926\n",
      "step: 300, loss: 0.058911170810461044\n",
      "step: 310, loss: 0.01855437271296978\n",
      "step: 320, loss: 0.0016094333259388804\n",
      "step: 330, loss: 0.06528449058532715\n",
      "step: 340, loss: 0.0014535118825733662\n",
      "step: 350, loss: 0.000432726665167138\n",
      "step: 360, loss: 0.0005195228150114417\n",
      "step: 370, loss: 0.04566948488354683\n",
      "step: 380, loss: 0.005394765175879002\n",
      "step: 390, loss: 0.03447425737977028\n",
      "step: 400, loss: 0.03043360635638237\n",
      "step: 410, loss: 0.0025220243260264397\n",
      "step: 420, loss: 0.00204424443654716\n",
      "step: 430, loss: 0.017428189516067505\n",
      "step: 440, loss: 0.000963857863098383\n",
      "step: 450, loss: 0.0013560125371441245\n",
      "step: 460, loss: 0.006512332241982222\n",
      "step: 470, loss: 0.003942468669265509\n",
      "step: 480, loss: 0.017209310084581375\n",
      "step: 490, loss: 0.003449846524745226\n",
      "step: 500, loss: 0.0019591092132031918\n",
      "step: 510, loss: 0.015600538812577724\n",
      "step: 520, loss: 0.006118993274867535\n",
      "step: 530, loss: 0.0867580845952034\n",
      "step: 540, loss: 0.015381637029349804\n",
      "step: 550, loss: 0.007769528776407242\n",
      "step: 560, loss: 0.0023986357264220715\n",
      "step: 570, loss: 0.001599657814949751\n",
      "step: 580, loss: 0.0401386022567749\n",
      "step: 590, loss: 0.0006808872567489743\n",
      "step: 600, loss: 0.023682858794927597\n",
      "step: 610, loss: 0.0004204794531688094\n",
      "step: 620, loss: 0.027190586552023888\n",
      "step: 630, loss: 0.005925873294472694\n",
      "step: 640, loss: 0.028697839006781578\n",
      "step: 650, loss: 0.01920422911643982\n",
      "step: 660, loss: 0.072359099984169\n",
      "step: 670, loss: 0.0355239063501358\n",
      "step: 680, loss: 0.03104753978550434\n",
      "step: 690, loss: 0.029207004234194756\n",
      "step: 700, loss: 0.01442225743085146\n",
      "step: 710, loss: 0.0036375722847878933\n",
      "step: 720, loss: 0.0009779123356565833\n",
      "step: 730, loss: 0.00034559553023427725\n",
      "step: 740, loss: 0.03064107708632946\n",
      "=========eval at epoch=11=========\n",
      "num_proposed:8546\n",
      "num_correct:8045\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.94\n",
      "f1=0.94\n",
      "weights were saved to checkpoints/01/11.pt\n",
      "=====sanity check======\n",
      "words: [CLS] It was the sixth year in a row that China avoided censure at the U.N. 's main human rights forum . [SEP]\n",
      "x: [  101  1135  1108  1103  3971  1214  1107   170  5105  1115  1975  9226\n",
      "   172  5026  3313  1120  1103   158   119   151   119   112   188  1514\n",
      "  1769  2266 13912   119   102]\n",
      "tokens: ['[CLS]', 'It', 'was', 'the', 'sixth', 'year', 'in', 'a', 'row', 'that', 'China', 'avoided', 'c', '##ens', '##ure', 'at', 'the', 'U', '.', 'N', '.', \"'\", 's', 'main', 'human', 'rights', 'forum', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 1 1 1 8 1 1 0 0 1 1 9 0 0 0 1 0 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O O B-LOC O O O O B-ORG O O O O O O <PAD>\n",
      "seqlen: 29\n",
      "=======================\n",
      "step: 0, loss: 0.012467549182474613\n",
      "step: 10, loss: 0.0017067348817363381\n",
      "step: 20, loss: 0.006839660461992025\n",
      "step: 30, loss: 0.0006165202939882874\n",
      "step: 40, loss: 0.003453320125117898\n",
      "step: 50, loss: 0.001188787748105824\n",
      "step: 60, loss: 0.011207426898181438\n",
      "step: 70, loss: 0.00357004813849926\n",
      "step: 80, loss: 0.012226931750774384\n",
      "step: 90, loss: 0.047029297798871994\n",
      "step: 100, loss: 0.005991058889776468\n",
      "step: 110, loss: 0.0028102053329348564\n",
      "step: 120, loss: 0.05196952074766159\n",
      "step: 130, loss: 0.0008332447614520788\n",
      "step: 140, loss: 0.000434121087891981\n",
      "step: 150, loss: 0.0321246013045311\n",
      "step: 160, loss: 0.018164176493883133\n",
      "step: 170, loss: 0.01505140122026205\n",
      "step: 180, loss: 0.0019598836079239845\n",
      "step: 190, loss: 0.006448946427553892\n",
      "step: 200, loss: 0.0011575741227716208\n",
      "step: 210, loss: 0.0006025759503245354\n",
      "step: 220, loss: 0.0006336032529361546\n",
      "step: 230, loss: 0.007640882395207882\n",
      "step: 240, loss: 0.004012716002762318\n",
      "step: 250, loss: 0.001945195603184402\n",
      "step: 260, loss: 0.01100765261799097\n",
      "step: 270, loss: 0.02610180154442787\n",
      "step: 280, loss: 0.044079095125198364\n",
      "step: 290, loss: 0.009037661366164684\n",
      "step: 300, loss: 0.02854185365140438\n",
      "step: 310, loss: 0.04770338162779808\n",
      "step: 320, loss: 0.05011408403515816\n",
      "step: 330, loss: 0.013888871297240257\n",
      "step: 340, loss: 0.008576746098697186\n",
      "step: 350, loss: 0.018851054832339287\n",
      "step: 360, loss: 0.04232644662261009\n",
      "step: 370, loss: 0.03931960463523865\n",
      "step: 380, loss: 0.008495206944644451\n",
      "step: 390, loss: 0.0383170023560524\n",
      "step: 400, loss: 0.003134914441034198\n",
      "step: 410, loss: 0.008630930446088314\n",
      "step: 420, loss: 0.03708566352725029\n",
      "step: 430, loss: 0.005097045097500086\n",
      "step: 440, loss: 0.011570453643798828\n",
      "step: 450, loss: 0.0024672220461070538\n",
      "step: 460, loss: 0.03683086857199669\n",
      "step: 470, loss: 0.0029793493449687958\n",
      "step: 480, loss: 0.01940050721168518\n",
      "step: 490, loss: 0.011860411614179611\n",
      "step: 500, loss: 0.05810215696692467\n",
      "step: 510, loss: 0.035541415214538574\n",
      "step: 520, loss: 0.04837251082062721\n",
      "step: 530, loss: 0.01924418844282627\n",
      "step: 540, loss: 0.0006804264849051833\n",
      "step: 550, loss: 0.0021640444174408913\n",
      "step: 560, loss: 0.025720320641994476\n",
      "step: 570, loss: 0.06123978644609451\n",
      "step: 580, loss: 0.006976282224059105\n",
      "step: 590, loss: 0.0007315540569834411\n",
      "step: 600, loss: 0.016087636351585388\n",
      "step: 610, loss: 0.006194233428686857\n",
      "step: 620, loss: 0.021179992705583572\n",
      "step: 630, loss: 0.01754254847764969\n",
      "step: 640, loss: 0.04544240981340408\n",
      "step: 650, loss: 0.006033406592905521\n",
      "step: 660, loss: 0.008873673155903816\n",
      "step: 670, loss: 0.03498333320021629\n",
      "step: 680, loss: 0.001240789657458663\n",
      "step: 690, loss: 0.07022597640752792\n",
      "step: 700, loss: 0.08764049410820007\n",
      "step: 710, loss: 0.001500746002420783\n",
      "=========eval at epoch=12=========\n",
      "num_proposed:8525\n",
      "num_correct:7987\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/12.pt\n",
      "=====sanity check======\n",
      "words: [CLS] International observers say the alleged irregularities could affect the outcome of voting for municipal assemblies . [SEP]\n",
      "x: [  101  1570 15793  1474  1103  6351 12692  4233  1180  6975  1103  9386\n",
      "  1104  6612  1111  5186 24798   119   102]\n",
      "tokens: ['[CLS]', 'International', 'observers', 'say', 'the', 'alleged', 'irregular', '##ities', 'could', 'affect', 'the', 'outcome', 'of', 'voting', 'for', 'municipal', 'assemblies', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O O O O O O O O O <PAD>\n",
      "seqlen: 19\n",
      "=======================\n",
      "step: 0, loss: 0.0032200468704104424\n",
      "step: 10, loss: 0.01645580306649208\n",
      "step: 20, loss: 0.0011239098384976387\n",
      "step: 30, loss: 0.0008682286133989692\n",
      "step: 40, loss: 0.0010576515924185514\n",
      "step: 50, loss: 0.0024405820295214653\n",
      "step: 60, loss: 0.008525023236870766\n",
      "step: 70, loss: 0.0030238148756325245\n",
      "step: 80, loss: 0.003962365444749594\n",
      "step: 90, loss: 0.0007094260072335601\n",
      "step: 100, loss: 0.07189062982797623\n",
      "step: 110, loss: 0.003350293729454279\n",
      "step: 120, loss: 0.00247033778578043\n",
      "step: 130, loss: 0.006119579076766968\n",
      "step: 140, loss: 0.030492160469293594\n",
      "step: 150, loss: 0.035664819180965424\n",
      "step: 160, loss: 0.023229608312249184\n",
      "step: 170, loss: 0.004600457847118378\n",
      "step: 180, loss: 0.023617926985025406\n",
      "step: 190, loss: 0.0404183603823185\n",
      "step: 200, loss: 0.0068820505402982235\n",
      "step: 210, loss: 0.02161206677556038\n",
      "step: 220, loss: 0.005048884078860283\n",
      "step: 230, loss: 0.011775772087275982\n",
      "step: 240, loss: 0.003812334965914488\n",
      "step: 250, loss: 0.011659128591418266\n",
      "step: 260, loss: 0.02850210852921009\n",
      "step: 270, loss: 0.004286009352654219\n",
      "step: 280, loss: 0.006868143565952778\n",
      "step: 290, loss: 0.02578858472406864\n",
      "step: 300, loss: 0.000572072749491781\n",
      "step: 310, loss: 0.029548123478889465\n",
      "step: 320, loss: 0.016688654199242592\n",
      "step: 330, loss: 0.031429506838321686\n",
      "step: 340, loss: 0.04916433244943619\n",
      "step: 350, loss: 0.06406848877668381\n",
      "step: 360, loss: 0.03944316878914833\n",
      "step: 370, loss: 0.13637524843215942\n",
      "step: 380, loss: 0.018091615289449692\n",
      "step: 390, loss: 0.001994589576497674\n",
      "step: 400, loss: 0.031232139095664024\n",
      "step: 410, loss: 0.012125085107982159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 420, loss: 0.03136926889419556\n",
      "step: 430, loss: 0.02324700355529785\n",
      "step: 440, loss: 0.007621020078659058\n",
      "step: 450, loss: 0.0662660002708435\n",
      "step: 460, loss: 0.005316449794918299\n",
      "step: 470, loss: 0.01739136502146721\n",
      "step: 480, loss: 0.007939671166241169\n",
      "step: 490, loss: 0.023798616603016853\n",
      "step: 500, loss: 0.00520484009757638\n",
      "step: 510, loss: 0.01281624287366867\n",
      "step: 520, loss: 0.020562440156936646\n",
      "step: 530, loss: 0.04854276776313782\n",
      "step: 540, loss: 0.011619487777352333\n",
      "step: 550, loss: 0.006516872439533472\n",
      "step: 560, loss: 0.12083683162927628\n",
      "step: 570, loss: 0.04584328830242157\n",
      "step: 580, loss: 0.004932920448482037\n",
      "step: 590, loss: 0.003931383602321148\n",
      "step: 600, loss: 0.01666448824107647\n",
      "step: 610, loss: 0.009469831362366676\n",
      "=========eval at epoch=13=========\n",
      "num_proposed:8542\n",
      "num_correct:7891\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/13.pt\n",
      "=====sanity check======\n",
      "words: [CLS] CRICKET - ENGLAND NAME SQUAD FOR ONE-DAY INTERNATIONALS . [SEP]\n",
      "x: [  101 15531  9741 22441  1942   118   142 11780 10783 16769   151 10964\n",
      "  2036   156  4880  2591 14569   143  9565 24497   118   141  1592  3663\n",
      " 15969 12880 15654 21669 11414 12507  1708   119   102]\n",
      "tokens: ['[CLS]', 'CR', '##IC', '##KE', '##T', '-', 'E', '##NG', '##LA', '##ND', 'N', '##AM', '##E', 'S', '##Q', '##U', '##AD', 'F', '##OR', 'ONE', '-', 'D', '##A', '##Y', 'IN', '##TE', '##RNA', '##TI', '##ON', '##AL', '##S', '.', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "y: [0 1 0 0 0 1 8 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0]\n",
      "tags: <PAD> O O B-LOC O O O O O O <PAD>\n",
      "seqlen: 33\n",
      "=======================\n",
      "step: 0, loss: 0.003523516934365034\n",
      "step: 10, loss: 0.0005758252809755504\n",
      "step: 20, loss: 0.025072775781154633\n",
      "step: 30, loss: 0.029500149190425873\n",
      "step: 40, loss: 0.006798616144806147\n",
      "step: 50, loss: 0.01719960942864418\n",
      "step: 60, loss: 0.005165366921573877\n",
      "step: 70, loss: 0.0022233822382986546\n",
      "step: 80, loss: 0.006208791397511959\n",
      "step: 90, loss: 0.015738246962428093\n",
      "step: 100, loss: 0.09016135334968567\n",
      "step: 110, loss: 0.054365936666727066\n",
      "step: 120, loss: 0.008380498737096786\n",
      "step: 130, loss: 0.050630658864974976\n",
      "step: 140, loss: 0.0006724642007611692\n",
      "step: 150, loss: 0.003585604950785637\n",
      "step: 160, loss: 0.0020020650699734688\n",
      "step: 170, loss: 0.0178536269813776\n",
      "step: 180, loss: 0.03267330303788185\n",
      "step: 190, loss: 0.018330441787838936\n",
      "step: 200, loss: 0.05415495112538338\n",
      "step: 210, loss: 0.004424357321113348\n",
      "step: 220, loss: 0.009376244619488716\n",
      "step: 230, loss: 0.0016863716300576925\n",
      "step: 240, loss: 0.01942175254225731\n",
      "step: 250, loss: 0.021198051050305367\n",
      "step: 260, loss: 0.0020586790051311255\n",
      "step: 270, loss: 0.0014351789141073823\n",
      "step: 280, loss: 0.020482730120420456\n",
      "step: 290, loss: 0.0014516115188598633\n",
      "step: 300, loss: 0.0011483849957585335\n",
      "step: 310, loss: 0.005810463335365057\n",
      "step: 320, loss: 0.0002729817060753703\n",
      "step: 330, loss: 0.004135738592594862\n",
      "step: 340, loss: 0.02356959506869316\n",
      "step: 350, loss: 0.02441594935953617\n",
      "step: 360, loss: 0.035075653344392776\n",
      "step: 370, loss: 0.010201211087405682\n",
      "step: 380, loss: 0.00039697965257801116\n",
      "step: 390, loss: 0.007794973440468311\n",
      "step: 400, loss: 0.020084641873836517\n",
      "step: 410, loss: 0.0017938793171197176\n",
      "step: 420, loss: 0.02448378875851631\n",
      "step: 430, loss: 0.014557834714651108\n",
      "step: 440, loss: 0.00561705743893981\n",
      "step: 450, loss: 0.02866794541478157\n",
      "step: 460, loss: 0.04176563769578934\n",
      "step: 470, loss: 0.02564295195043087\n",
      "step: 480, loss: 0.03798358514904976\n",
      "step: 490, loss: 0.001661601709201932\n",
      "step: 500, loss: 0.049855757504701614\n",
      "step: 510, loss: 0.020853741094470024\n",
      "step: 520, loss: 0.035022538155317307\n",
      "step: 530, loss: 0.03959719091653824\n",
      "step: 540, loss: 0.006285871844738722\n",
      "step: 550, loss: 0.014149446040391922\n",
      "=========eval at epoch=14=========\n",
      "num_proposed:8605\n",
      "num_correct:7932\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/14.pt\n",
      "=====sanity check======\n",
      "words: [CLS] A state police spokeswoman said there were reports of minor injuries as a result of the derailment near Roxbury , a small town on the edge of the Northfield Mountains some 15 miles southwest of Montpelier , the state capital . [SEP]\n",
      "x: [  101   138  1352  2021  2910  1116  9462  1163  1175  1127  3756  1104\n",
      "  3137  5917  1112   170  1871  1104  1103  4167 11922  1880  1485   155\n",
      " 10649  4109   117   170  1353  1411  1113  1103  2652  1104  1103  1456\n",
      "  2427  5249  1199  1405  1829  5090  1104 20018 10522  2852   117  1103\n",
      "  1352  2364   119   102]\n",
      "tokens: ['[CLS]', 'A', 'state', 'police', 'spoke', '##s', '##woman', 'said', 'there', 'were', 'reports', 'of', 'minor', 'injuries', 'as', 'a', 'result', 'of', 'the', 'der', '##ail', '##ment', 'near', 'R', '##ox', '##bury', ',', 'a', 'small', 'town', 'on', 'the', 'edge', 'of', 'the', 'North', '##field', 'Mountains', 'some', '15', 'miles', 'southwest', 'of', 'Mont', '##pel', '##ier', ',', 'the', 'state', 'capital', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 8 0 0 1 1 1 1 1 1 1 1 1 8 0\n",
      " 2 1 1 1 1 1 8 0 0 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O B-LOC I-LOC O O O O O B-LOC O O O O O <PAD>\n",
      "seqlen: 52\n",
      "=======================\n",
      "step: 0, loss: 0.011700581759214401\n",
      "step: 10, loss: 0.005861916579306126\n",
      "step: 20, loss: 0.02441314049065113\n",
      "step: 30, loss: 0.013876178301870823\n",
      "step: 40, loss: 0.0005199106526561081\n",
      "step: 50, loss: 0.009213346987962723\n",
      "step: 60, loss: 0.05175933241844177\n",
      "step: 70, loss: 0.0026772315613925457\n",
      "step: 80, loss: 0.012225262820720673\n",
      "step: 90, loss: 0.008383541367948055\n",
      "step: 100, loss: 0.0022064880467951298\n",
      "step: 110, loss: 0.0019889026880264282\n",
      "step: 120, loss: 0.002368661342188716\n",
      "step: 130, loss: 0.0015758724184706807\n",
      "step: 140, loss: 0.03863196074962616\n",
      "step: 150, loss: 0.0021358951926231384\n",
      "step: 160, loss: 0.001019269460812211\n",
      "step: 170, loss: 0.00461097527295351\n",
      "step: 180, loss: 0.04844222217798233\n",
      "step: 190, loss: 0.002659125719219446\n",
      "step: 200, loss: 0.002357347169891\n",
      "step: 210, loss: 0.03217862918972969\n",
      "step: 220, loss: 0.009584207087755203\n",
      "step: 230, loss: 0.029480237513780594\n",
      "step: 240, loss: 0.005679408088326454\n",
      "step: 250, loss: 0.03879094868898392\n",
      "step: 260, loss: 0.023845039308071136\n",
      "step: 270, loss: 0.003978111315518618\n",
      "step: 280, loss: 0.029788846150040627\n",
      "step: 290, loss: 0.0037202262319624424\n",
      "step: 300, loss: 0.0008811498992145061\n",
      "step: 310, loss: 0.005654481705278158\n",
      "step: 320, loss: 0.0030598798766732216\n",
      "=========eval at epoch=15=========\n",
      "num_proposed:8508\n",
      "num_correct:7968\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/15.pt\n",
      "=====sanity check======\n",
      "words: [CLS] WELLINGTON 1996-08-22 [SEP]\n",
      "x: [  101   160 21678  2162 15740 18082  2249  1820   118  4775   118  1659\n",
      "   102]\n",
      "tokens: ['[CLS]', 'W', '##EL', '##L', '##ING', '##TO', '##N', '1996', '-', '08', '-', '22', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "y: [0 8 0 0 0 0 0 1 0 0 0 0 0]\n",
      "tags: <PAD> B-LOC O <PAD>\n",
      "seqlen: 13\n",
      "=======================\n",
      "step: 0, loss: 0.0006772245396859944\n",
      "step: 10, loss: 0.007213316857814789\n",
      "step: 20, loss: 0.0015712506137788296\n",
      "step: 30, loss: 0.042649101465940475\n",
      "step: 40, loss: 0.0008431387832388282\n",
      "step: 50, loss: 0.018658030778169632\n",
      "step: 60, loss: 0.0031178563367575407\n",
      "step: 70, loss: 0.01580246165394783\n",
      "step: 80, loss: 0.0003261913952883333\n",
      "step: 90, loss: 0.0036059224512428045\n",
      "step: 100, loss: 0.01372193917632103\n",
      "step: 110, loss: 0.0013302345760166645\n",
      "step: 120, loss: 0.0006581572815775871\n",
      "step: 130, loss: 0.005504219327121973\n",
      "step: 140, loss: 0.003678720211610198\n",
      "step: 150, loss: 0.00044444913510233164\n",
      "step: 160, loss: 0.03339178115129471\n",
      "step: 170, loss: 0.001192730967886746\n",
      "step: 180, loss: 0.0005619923467747867\n",
      "step: 190, loss: 0.010775615461170673\n",
      "step: 200, loss: 0.1427488923072815\n",
      "step: 210, loss: 0.028883470222353935\n",
      "step: 220, loss: 0.0006260978407226503\n",
      "step: 230, loss: 0.013051030226051807\n",
      "step: 240, loss: 0.01574409380555153\n",
      "step: 250, loss: 0.0011405734112486243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 260, loss: 0.019754916429519653\n",
      "step: 270, loss: 0.06178651005029678\n",
      "=========eval at epoch=16=========\n",
      "num_proposed:8529\n",
      "num_correct:7940\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.92\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/16.pt\n",
      "=====sanity check======\n",
      "words: [CLS] - Iraq 's President Saddam Hussein meets with chairman of the Russian liberal democratic party Vladimir Zhirinovsky . [SEP]\n",
      "x: [  101   118  5008   112   188  1697 27091 17605  5636  1114  3931  1104\n",
      "  1103  1938  7691  9327  1710  8591   163 14518  4559 13510   119   102]\n",
      "tokens: ['[CLS]', '-', 'Iraq', \"'\", 's', 'President', 'Saddam', 'Hussein', 'meets', 'with', 'chairman', 'of', 'the', 'Russian', 'liberal', 'democratic', 'party', 'Vladimir', 'Z', '##hir', '##ino', '##vsky', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "y: [0 1 8 1 0 1 3 4 1 1 1 1 1 7 1 1 1 3 4 0 0 0 1 0]\n",
      "tags: <PAD> O B-LOC O O B-PER I-PER O O O O O B-MISC O O O B-PER I-PER O <PAD>\n",
      "seqlen: 24\n",
      "=======================\n",
      "step: 0, loss: 0.01958620920777321\n",
      "step: 10, loss: 0.005832725204527378\n",
      "step: 20, loss: 0.019550541415810585\n",
      "step: 30, loss: 0.0061787450686097145\n",
      "step: 40, loss: 0.0014617301058024168\n",
      "step: 50, loss: 0.0013423928758129478\n",
      "step: 60, loss: 0.007468081545084715\n",
      "step: 70, loss: 0.016382185742259026\n",
      "step: 80, loss: 0.009508044458925724\n",
      "step: 90, loss: 0.09449148923158646\n",
      "step: 100, loss: 0.0028791131917387247\n",
      "step: 110, loss: 0.03912251070141792\n",
      "step: 120, loss: 0.0014670260716229677\n",
      "step: 130, loss: 0.012638702988624573\n",
      "step: 140, loss: 0.00036906154127791524\n",
      "step: 150, loss: 0.00529148755595088\n",
      "step: 160, loss: 0.02658047154545784\n",
      "step: 170, loss: 0.018934419378638268\n",
      "step: 180, loss: 0.00147305300924927\n",
      "step: 190, loss: 0.03719930723309517\n",
      "step: 200, loss: 0.00926664937287569\n",
      "step: 210, loss: 0.05382134020328522\n",
      "step: 220, loss: 0.001406333758495748\n",
      "step: 230, loss: 0.014813899993896484\n",
      "step: 240, loss: 0.013171752914786339\n",
      "step: 250, loss: 0.029679467901587486\n",
      "step: 260, loss: 0.111695796251297\n",
      "step: 270, loss: 0.004572712350636721\n",
      "=========eval at epoch=17=========\n",
      "num_proposed:8555\n",
      "num_correct:7989\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/17.pt\n",
      "=====sanity check======\n",
      "words: [CLS] U.S. Treasury balances at Fed rose on Aug 27 . [SEP]\n",
      "x: [  101   158   119   156   119 11712  5233  1116  1120 26356  3152  1113\n",
      " 16892  1765   119   102]\n",
      "tokens: ['[CLS]', 'U', '.', 'S', '.', 'Treasury', 'balance', '##s', 'at', 'Fed', 'rose', 'on', 'Aug', '27', '.', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 9 0 0 0 5 1 0 1 9 1 1 1 1 1 0]\n",
      "tags: <PAD> B-ORG I-ORG O O B-ORG O O O O O <PAD>\n",
      "seqlen: 16\n",
      "=======================\n",
      "step: 0, loss: 0.0015794894425198436\n",
      "step: 10, loss: 0.008083636872470379\n",
      "step: 20, loss: 0.0016056181630119681\n",
      "step: 30, loss: 0.004321333486586809\n",
      "step: 40, loss: 0.0004310502845328301\n",
      "step: 50, loss: 0.010112238116562366\n",
      "step: 60, loss: 0.04058929160237312\n",
      "step: 70, loss: 0.004206312820315361\n",
      "step: 80, loss: 0.08805414289236069\n",
      "step: 90, loss: 0.0015634619630873203\n",
      "step: 100, loss: 0.03147868067026138\n",
      "step: 110, loss: 0.03280196711421013\n",
      "step: 120, loss: 0.0004982982645742595\n",
      "step: 130, loss: 0.0021546087227761745\n",
      "step: 140, loss: 0.00329710659570992\n",
      "step: 150, loss: 0.0015550550306215882\n",
      "step: 160, loss: 0.0018674071179702878\n",
      "step: 170, loss: 0.026488646864891052\n",
      "step: 180, loss: 0.04406159743666649\n",
      "step: 190, loss: 0.006852127145975828\n",
      "step: 200, loss: 0.002270393306389451\n",
      "step: 210, loss: 0.034369029104709625\n",
      "step: 220, loss: 0.008262420073151588\n",
      "step: 230, loss: 0.004618772305548191\n",
      "step: 240, loss: 0.0781213641166687\n",
      "step: 250, loss: 0.0003617482434492558\n",
      "step: 260, loss: 0.012359133921563625\n",
      "step: 270, loss: 0.044500380754470825\n",
      "step: 280, loss: 0.0031797396950423717\n",
      "step: 290, loss: 0.03871174529194832\n",
      "step: 300, loss: 0.0012070093071088195\n",
      "step: 310, loss: 0.03578440472483635\n",
      "step: 320, loss: 0.0054411860182881355\n",
      "step: 330, loss: 0.0075277225114405155\n",
      "step: 340, loss: 0.006283437833189964\n",
      "step: 350, loss: 0.027531055733561516\n",
      "step: 360, loss: 0.05900063365697861\n",
      "step: 370, loss: 0.026239732280373573\n",
      "step: 380, loss: 0.044430218636989594\n",
      "step: 390, loss: 0.025193603709340096\n",
      "step: 400, loss: 0.017405301332473755\n",
      "step: 410, loss: 0.0344550795853138\n",
      "step: 420, loss: 0.003100745379924774\n",
      "step: 430, loss: 0.022215085104107857\n",
      "step: 440, loss: 0.001584631041623652\n",
      "step: 450, loss: 0.1140715479850769\n",
      "step: 460, loss: 0.0011418936774134636\n",
      "step: 470, loss: 0.0397859551012516\n",
      "step: 480, loss: 0.0036412274930626154\n",
      "step: 490, loss: 0.004156559705734253\n",
      "step: 500, loss: 0.060888517647981644\n",
      "step: 510, loss: 0.03156455606222153\n",
      "step: 520, loss: 0.0040668402798473835\n",
      "step: 530, loss: 0.00023966623120941222\n",
      "step: 540, loss: 0.015084562823176384\n",
      "=========eval at epoch=18=========\n",
      "num_proposed:8597\n",
      "num_correct:7941\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/18.pt\n",
      "=====sanity check======\n",
      "words: [CLS] ATLANTA AT PITTSBURGH [SEP]\n",
      "x: [  101 13020 10783 15681  1592 13020   153 12150 11365  2064 19556  2349\n",
      "  3048   102]\n",
      "tokens: ['[CLS]', 'AT', '##LA', '##NT', '##A', 'AT', 'P', '##IT', '##TS', '##B', '##UR', '##G', '##H', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "y: [0 9 0 0 0 1 8 0 0 0 0 0 0 0]\n",
      "tags: <PAD> B-ORG O B-LOC <PAD>\n",
      "seqlen: 14\n",
      "=======================\n",
      "step: 0, loss: 0.0004270359640941024\n",
      "step: 10, loss: 0.019596561789512634\n",
      "step: 20, loss: 0.011360861361026764\n",
      "step: 30, loss: 0.027119392529129982\n",
      "step: 40, loss: 0.007554175332188606\n",
      "step: 50, loss: 0.024046404287219048\n",
      "step: 60, loss: 0.007756730075925589\n",
      "step: 70, loss: 0.11245512962341309\n",
      "step: 80, loss: 0.03034990094602108\n",
      "step: 90, loss: 0.02043677121400833\n",
      "step: 100, loss: 0.00141971989069134\n",
      "step: 110, loss: 0.01933469995856285\n",
      "step: 120, loss: 0.0019039633916690946\n",
      "step: 130, loss: 0.0028424737975001335\n",
      "step: 140, loss: 0.01719455048441887\n",
      "=========eval at epoch=19=========\n",
      "num_proposed:8586\n",
      "num_correct:8007\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/19.pt\n",
      "=====sanity check======\n",
      "words: [CLS] Attendance : 2,189 [SEP]\n",
      "x: [  101  6868   131   123   117 22415   102]\n",
      "tokens: ['[CLS]', 'Attendance', ':', '2', ',', '189', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 0, 0, 1]\n",
      "y: [0 1 1 1 0 0 0]\n",
      "tags: <PAD> O O O <PAD>\n",
      "seqlen: 7\n",
      "=======================\n",
      "step: 0, loss: 0.0012768516317009926\n",
      "step: 10, loss: 0.04586484283208847\n",
      "step: 20, loss: 0.04851435497403145\n",
      "step: 30, loss: 0.0018760728416964412\n",
      "step: 40, loss: 0.0017532388446852565\n",
      "step: 50, loss: 0.019585002213716507\n",
      "step: 60, loss: 0.0003768375900108367\n",
      "step: 70, loss: 0.00047893376904539764\n",
      "step: 80, loss: 0.006445085629820824\n",
      "step: 90, loss: 0.001573947025462985\n",
      "step: 100, loss: 0.0003495176206342876\n",
      "step: 110, loss: 0.0029760864563286304\n",
      "step: 120, loss: 0.01124268863350153\n",
      "step: 130, loss: 0.0016120100626721978\n",
      "step: 140, loss: 0.027448654174804688\n",
      "step: 150, loss: 0.002332039875909686\n",
      "step: 160, loss: 0.0016274616355076432\n",
      "step: 170, loss: 0.0014911048347130418\n",
      "step: 180, loss: 0.024295834824442863\n",
      "step: 190, loss: 0.003802255494520068\n",
      "step: 200, loss: 0.0007443229551427066\n",
      "step: 210, loss: 0.003511994844302535\n",
      "step: 220, loss: 0.0010847481898963451\n",
      "step: 230, loss: 0.0002658484736457467\n",
      "step: 240, loss: 0.0017971156630665064\n",
      "step: 250, loss: 0.0004241141432430595\n",
      "step: 260, loss: 0.00539725087583065\n",
      "step: 270, loss: 0.021470343694090843\n",
      "step: 280, loss: 0.025517337024211884\n",
      "step: 290, loss: 0.012976902537047863\n",
      "step: 300, loss: 0.0017469875747337937\n",
      "step: 310, loss: 0.011207534931600094\n",
      "step: 320, loss: 0.0036275724414736032\n",
      "step: 330, loss: 0.0016443312633782625\n",
      "step: 340, loss: 0.003731789533048868\n",
      "step: 350, loss: 0.018283963203430176\n",
      "step: 360, loss: 0.027392402291297913\n",
      "step: 370, loss: 0.0053081996738910675\n",
      "step: 380, loss: 0.0008557690307497978\n",
      "step: 390, loss: 0.013241092674434185\n",
      "step: 400, loss: 0.006598348263651133\n",
      "step: 410, loss: 0.018540896475315094\n",
      "step: 420, loss: 0.02960205078125\n",
      "step: 430, loss: 0.004691665526479483\n",
      "step: 440, loss: 0.06885995715856552\n",
      "step: 450, loss: 0.01718270778656006\n",
      "step: 460, loss: 0.01747697964310646\n",
      "step: 470, loss: 0.00032214669045060873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 480, loss: 0.007322611287236214\n",
      "step: 490, loss: 0.007955797016620636\n",
      "step: 500, loss: 0.002873152494430542\n",
      "step: 510, loss: 0.019372571259737015\n",
      "step: 520, loss: 0.00584058603271842\n",
      "step: 530, loss: 0.039576850831508636\n",
      "step: 540, loss: 0.0032230867072939873\n",
      "step: 550, loss: 0.05144813284277916\n",
      "step: 560, loss: 0.023352615535259247\n",
      "step: 570, loss: 0.026649853214621544\n",
      "step: 580, loss: 0.04052000492811203\n",
      "step: 590, loss: 0.03182521462440491\n",
      "step: 600, loss: 0.02291063778102398\n",
      "step: 610, loss: 0.026354433968663216\n",
      "step: 620, loss: 0.0012637615436688066\n",
      "step: 630, loss: 0.0021424756851047277\n",
      "step: 640, loss: 0.007155577186495066\n",
      "step: 650, loss: 0.0031727852765470743\n",
      "step: 660, loss: 0.021655945107340813\n",
      "step: 670, loss: 0.05291448161005974\n",
      "step: 680, loss: 0.010833803564310074\n",
      "step: 690, loss: 0.017100568860769272\n",
      "step: 700, loss: 0.028967885300517082\n",
      "step: 710, loss: 0.013795576058328152\n",
      "step: 720, loss: 0.0031148765701800585\n",
      "step: 730, loss: 0.004249683581292629\n",
      "step: 740, loss: 0.08546245843172073\n",
      "step: 750, loss: 0.003852981375530362\n",
      "step: 760, loss: 0.0022583461832255125\n",
      "step: 770, loss: 0.01673578843474388\n",
      "=========eval at epoch=20=========\n",
      "num_proposed:8462\n",
      "num_correct:7920\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.92\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/20.pt\n",
      "=====sanity check======\n",
      "words: [CLS] ( million A$ unless stated ) [SEP]\n",
      "x: [ 101  113 1550  138  109 4895 2202  114  102]\n",
      "tokens: ['[CLS]', '(', 'million', 'A', '$', 'unless', 'stated', ')', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "y: [0 1 1 7 0 1 1 1 0]\n",
      "tags: <PAD> O O B-MISC O O O <PAD>\n",
      "seqlen: 9\n",
      "=======================\n",
      "step: 0, loss: 0.10668619722127914\n",
      "step: 10, loss: 0.010207642801105976\n",
      "step: 20, loss: 0.016615916043519974\n",
      "step: 30, loss: 0.002507178345695138\n",
      "step: 40, loss: 0.0026411768049001694\n",
      "step: 50, loss: 0.005249174311757088\n",
      "step: 60, loss: 0.006024351343512535\n",
      "step: 70, loss: 0.0017082984559237957\n",
      "step: 80, loss: 0.0035420202184468508\n",
      "step: 90, loss: 0.029826831072568893\n",
      "step: 100, loss: 0.0020254850387573242\n",
      "step: 110, loss: 0.0019256829982623458\n",
      "step: 120, loss: 0.0038842603098601103\n",
      "step: 130, loss: 0.02654447592794895\n",
      "step: 140, loss: 0.0008327597170136869\n",
      "step: 150, loss: 0.0006982448394410312\n",
      "step: 160, loss: 0.002751592779532075\n",
      "step: 170, loss: 0.0048667858354747295\n",
      "step: 180, loss: 0.0004206532903481275\n",
      "step: 190, loss: 0.05684603005647659\n",
      "step: 200, loss: 0.008323650807142258\n",
      "step: 210, loss: 0.0006517876754514873\n",
      "step: 220, loss: 0.030249664559960365\n",
      "step: 230, loss: 0.03305813670158386\n",
      "step: 240, loss: 0.0029309201054275036\n",
      "step: 250, loss: 0.0002921147970482707\n",
      "step: 260, loss: 0.005595341324806213\n",
      "step: 270, loss: 0.0041819242760539055\n",
      "step: 280, loss: 0.0002735354646574706\n",
      "step: 290, loss: 0.006087887566536665\n",
      "step: 300, loss: 0.005989007651805878\n",
      "step: 310, loss: 0.04896058142185211\n",
      "step: 320, loss: 0.010687354952096939\n",
      "step: 330, loss: 0.02713179402053356\n",
      "step: 340, loss: 0.0246320478618145\n",
      "step: 350, loss: 0.0015163847710937262\n",
      "step: 360, loss: 0.000754653534386307\n",
      "step: 370, loss: 0.0032569889444857836\n",
      "step: 380, loss: 0.06683526933193207\n",
      "step: 390, loss: 0.02701045572757721\n",
      "step: 400, loss: 0.0012994077987968922\n",
      "step: 410, loss: 0.003186641726642847\n",
      "step: 420, loss: 0.011241018772125244\n",
      "step: 430, loss: 0.004320657346397638\n",
      "step: 440, loss: 0.045456044375896454\n",
      "step: 450, loss: 0.007008278276771307\n",
      "step: 460, loss: 0.00849510170519352\n",
      "step: 470, loss: 0.008066138252615929\n",
      "step: 480, loss: 0.01715180091559887\n",
      "step: 490, loss: 0.012023690156638622\n",
      "step: 500, loss: 0.019821468740701675\n",
      "step: 510, loss: 0.02310904860496521\n",
      "step: 520, loss: 0.033598821610212326\n",
      "step: 530, loss: 0.014889492653310299\n",
      "step: 540, loss: 0.09024344384670258\n",
      "step: 550, loss: 0.017677228897809982\n",
      "step: 560, loss: 0.027446510270237923\n",
      "step: 570, loss: 0.0005360367940738797\n",
      "step: 580, loss: 0.0009651620639488101\n",
      "step: 590, loss: 0.03199353441596031\n",
      "step: 600, loss: 0.09279123693704605\n",
      "step: 610, loss: 0.017921488732099533\n",
      "step: 620, loss: 0.003694073762744665\n",
      "step: 630, loss: 0.0022556812036782503\n",
      "=========eval at epoch=21=========\n",
      "num_proposed:8565\n",
      "num_correct:7942\n",
      "num_gold:8603\n",
      "precision=0.93\n",
      "recall=0.92\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/21.pt\n",
      "=====sanity check======\n",
      "words: [CLS] It has since been silent on the issue . [SEP]\n",
      "x: [ 101 1135 1144 1290 1151 3826 1113 1103 2486  119  102]\n",
      "tokens: ['[CLS]', 'It', 'has', 'since', 'been', 'silent', 'on', 'the', 'issue', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O O <PAD>\n",
      "seqlen: 11\n",
      "=======================\n",
      "step: 0, loss: 0.005511179566383362\n",
      "step: 10, loss: 0.006942735984921455\n",
      "step: 20, loss: 0.0005597919807769358\n",
      "step: 30, loss: 0.02143898420035839\n",
      "step: 40, loss: 0.0008282628259621561\n",
      "step: 50, loss: 0.00197180500254035\n",
      "step: 60, loss: 0.012558038346469402\n",
      "step: 70, loss: 0.0007317911367863417\n",
      "step: 80, loss: 0.006897835060954094\n",
      "step: 90, loss: 0.016041409224271774\n",
      "step: 100, loss: 0.0019520752830430865\n",
      "step: 110, loss: 0.0009519922314211726\n",
      "step: 120, loss: 0.0006164067308418453\n",
      "step: 130, loss: 0.0026672291569411755\n",
      "step: 140, loss: 0.01669052429497242\n",
      "step: 150, loss: 0.04903142526745796\n",
      "step: 160, loss: 0.0014083620626479387\n",
      "step: 170, loss: 0.0022554369643330574\n",
      "step: 180, loss: 0.015198515728116035\n",
      "step: 190, loss: 0.004687770269811153\n",
      "step: 200, loss: 0.0014331949641928077\n",
      "step: 210, loss: 0.01805008202791214\n",
      "step: 220, loss: 0.007084900513291359\n",
      "step: 230, loss: 0.02324424497783184\n",
      "step: 240, loss: 0.0073183393105864525\n",
      "step: 250, loss: 0.0009832751238718629\n",
      "step: 260, loss: 0.008773709647357464\n",
      "step: 270, loss: 0.05320035293698311\n",
      "step: 280, loss: 0.0014673746190965176\n",
      "step: 290, loss: 0.06578443944454193\n",
      "step: 300, loss: 0.00043062784243375063\n",
      "step: 310, loss: 0.013504480943083763\n",
      "step: 320, loss: 0.0010268447222188115\n",
      "step: 330, loss: 0.023918412625789642\n",
      "step: 340, loss: 0.07212017476558685\n",
      "step: 350, loss: 0.019411971792578697\n",
      "step: 360, loss: 0.0005152194644324481\n",
      "step: 370, loss: 0.0024031680077314377\n",
      "step: 380, loss: 0.006247625686228275\n",
      "step: 390, loss: 0.012096576392650604\n",
      "step: 400, loss: 0.0017307462403550744\n",
      "step: 410, loss: 0.012432410381734371\n",
      "step: 420, loss: 0.0004424022336024791\n",
      "step: 430, loss: 0.002108623506501317\n",
      "step: 440, loss: 0.03435453772544861\n",
      "step: 450, loss: 0.013761882670223713\n",
      "step: 460, loss: 0.02254040166735649\n",
      "step: 470, loss: 0.002930261194705963\n",
      "step: 480, loss: 0.03900405764579773\n",
      "step: 490, loss: 0.0074943192303180695\n",
      "step: 500, loss: 0.018301887437701225\n",
      "step: 510, loss: 0.003371620550751686\n",
      "step: 520, loss: 0.00347894080914557\n",
      "step: 530, loss: 0.04009399190545082\n",
      "step: 540, loss: 0.015799423679709435\n",
      "step: 550, loss: 0.0004056005855090916\n",
      "step: 560, loss: 0.012597682885825634\n",
      "step: 570, loss: 0.00020953883358743042\n",
      "step: 580, loss: 0.004336862359195948\n",
      "step: 590, loss: 0.0012343917042016983\n",
      "step: 600, loss: 0.0005538960103876889\n",
      "step: 610, loss: 0.007569018751382828\n",
      "step: 620, loss: 0.001288638566620648\n",
      "step: 630, loss: 0.0004031299613416195\n",
      "step: 640, loss: 0.002200988819822669\n",
      "step: 650, loss: 0.04109913483262062\n",
      "step: 660, loss: 0.0029603487346321344\n",
      "step: 670, loss: 0.0032383936922997236\n",
      "step: 680, loss: 0.0008318307809531689\n",
      "step: 690, loss: 0.002060558646917343\n",
      "step: 700, loss: 0.00442884024232626\n",
      "step: 710, loss: 0.031111430376768112\n",
      "step: 720, loss: 0.01168020348995924\n",
      "step: 730, loss: 0.0008972919313237071\n",
      "step: 740, loss: 0.0008521488634869456\n",
      "step: 750, loss: 0.028220275416970253\n",
      "=========eval at epoch=22=========\n",
      "num_proposed:8543\n",
      "num_correct:7995\n",
      "num_gold:8603\n",
      "precision=0.94\n",
      "recall=0.93\n",
      "f1=0.93\n",
      "weights were saved to checkpoints/01/22.pt\n",
      "=====sanity check======\n",
      "words: [CLS] Rugby star once linked to Princess Diana divorces . [SEP]\n",
      "x: [ 101 5457 2851 1517 5128 1106 4738 8506 8126 1116  119  102]\n",
      "tokens: ['[CLS]', 'Rugby', 'star', 'once', 'linked', 'to', 'Princess', 'Diana', 'divorce', '##s', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 3 1 0 1 0]\n",
      "tags: <PAD> O O O O O O B-PER O O <PAD>\n",
      "seqlen: 12\n",
      "=======================\n",
      "step: 0, loss: 0.007481222972273827\n",
      "step: 10, loss: 0.0017611111979931593\n",
      "step: 20, loss: 0.00018433645891491324\n",
      "step: 30, loss: 0.0003393600054550916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 40, loss: 0.0006269299774430692\n",
      "step: 50, loss: 0.000541826942935586\n",
      "step: 60, loss: 0.000659323763102293\n",
      "step: 70, loss: 0.0004546412965282798\n",
      "step: 80, loss: 0.00018400134285911918\n",
      "step: 90, loss: 0.010298162698745728\n",
      "step: 100, loss: 0.0032770789694041014\n",
      "step: 110, loss: 0.02961757965385914\n",
      "step: 120, loss: 0.0007046579848974943\n",
      "step: 130, loss: 0.0004226487362757325\n",
      "step: 140, loss: 0.0005355527391657233\n",
      "step: 150, loss: 0.0013838743325322866\n",
      "step: 160, loss: 0.0022498841863125563\n",
      "step: 170, loss: 0.001424493850208819\n",
      "step: 180, loss: 0.0020515467040240765\n",
      "step: 190, loss: 0.001409244490787387\n",
      "step: 200, loss: 0.02602306194603443\n",
      "step: 210, loss: 0.018150173127651215\n",
      "step: 220, loss: 0.041245218366384506\n",
      "step: 230, loss: 0.02516542375087738\n",
      "step: 240, loss: 0.0009531128453090787\n",
      "step: 250, loss: 0.01672859489917755\n",
      "step: 260, loss: 0.11258979886770248\n",
      "step: 270, loss: 0.00412875646725297\n",
      "step: 280, loss: 0.0020482358522713184\n",
      "step: 290, loss: 0.007213279139250517\n",
      "step: 300, loss: 0.002264107810333371\n",
      "step: 310, loss: 0.02360612340271473\n",
      "step: 320, loss: 0.03175685182213783\n",
      "step: 330, loss: 0.00075141730485484\n",
      "step: 340, loss: 0.0004684360756073147\n",
      "step: 350, loss: 0.0008474416681565344\n",
      "step: 360, loss: 0.0023804737720638514\n",
      "step: 370, loss: 0.0021850571502000093\n",
      "step: 380, loss: 0.04107695817947388\n",
      "step: 390, loss: 0.005942420102655888\n",
      "step: 400, loss: 0.0024123683106154203\n",
      "step: 410, loss: 0.00024178317107725888\n",
      "step: 420, loss: 0.0003007326740771532\n",
      "step: 430, loss: 0.0007182342233136296\n",
      "step: 440, loss: 0.0018374330829828978\n",
      "step: 450, loss: 0.021477172151207924\n",
      "step: 460, loss: 0.0007644473807886243\n",
      "step: 470, loss: 0.0011978488182649016\n",
      "step: 480, loss: 0.0005895011709071696\n",
      "step: 490, loss: 0.006175274029374123\n",
      "step: 500, loss: 0.045129746198654175\n",
      "step: 510, loss: 0.05283578485250473\n",
      "step: 520, loss: 0.010029039345681667\n",
      "step: 530, loss: 0.006395124830305576\n",
      "step: 540, loss: 0.002131703309714794\n",
      "step: 550, loss: 0.10160835087299347\n",
      "step: 560, loss: 0.0171105545014143\n",
      "step: 570, loss: 0.0012085805647075176\n",
      "step: 580, loss: 0.014911451376974583\n",
      "step: 590, loss: 0.000281825807178393\n",
      "step: 600, loss: 0.001606823643669486\n",
      "step: 610, loss: 0.0013404961209744215\n",
      "step: 620, loss: 0.006302892696112394\n",
      "step: 630, loss: 0.02301071584224701\n",
      "step: 640, loss: 0.004982070531696081\n",
      "step: 650, loss: 0.04045763984322548\n",
      "step: 660, loss: 0.01396880391985178\n",
      "step: 670, loss: 0.0009507165523245931\n",
      "step: 680, loss: 0.0002773425367195159\n",
      "step: 690, loss: 0.002535890555009246\n",
      "step: 700, loss: 0.0008027160656638443\n",
      "step: 710, loss: 0.012809919193387032\n",
      "step: 720, loss: 0.0025618274230509996\n",
      "step: 730, loss: 0.03793502599000931\n",
      "step: 740, loss: 0.0014330162666738033\n",
      "step: 750, loss: 0.012112168595194817\n",
      "step: 760, loss: 0.028983784839510918\n",
      "step: 770, loss: 0.0031036187428981066\n",
      "step: 780, loss: 0.003900306299328804\n",
      "step: 790, loss: 0.028510775417089462\n",
      "step: 800, loss: 0.0015962290344759822\n",
      "step: 810, loss: 0.01642737351357937\n",
      "step: 820, loss: 0.0007515940815210342\n",
      "step: 830, loss: 0.01136402040719986\n",
      "step: 840, loss: 0.021029090508818626\n",
      "step: 850, loss: 0.018473081290721893\n",
      "step: 860, loss: 0.0013257141690701246\n",
      "step: 870, loss: 0.001699086744338274\n",
      "step: 880, loss: 0.011126668192446232\n",
      "step: 890, loss: 0.011906365863978863\n",
      "step: 900, loss: 0.004463966470211744\n",
      "step: 910, loss: 0.0007199877290986478\n",
      "=========eval at epoch=23=========\n",
      "num_proposed:8588\n",
      "num_correct:7943\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/23.pt\n",
      "=====sanity check======\n",
      "words: [CLS] Port Vale 3 0 2 1 2 4 2 [SEP]\n",
      "x: [  101  3905 10532   124   121   123   122   123   125   123   102]\n",
      "tokens: ['[CLS]', 'Port', 'Vale', '3', '0', '2', '1', '2', '4', '2', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 9 5 1 1 1 1 1 1 1 0]\n",
      "tags: <PAD> B-ORG I-ORG O O O O O O O <PAD>\n",
      "seqlen: 11\n",
      "=======================\n",
      "step: 0, loss: 0.0003824255836661905\n",
      "step: 10, loss: 0.009863068349659443\n",
      "step: 20, loss: 0.0066336072050035\n",
      "step: 30, loss: 0.0075318943709135056\n",
      "step: 40, loss: 0.011633429676294327\n",
      "step: 50, loss: 0.0024890985805541277\n",
      "step: 60, loss: 0.0025346537586301565\n",
      "step: 70, loss: 0.0019331815419718623\n",
      "step: 80, loss: 0.017750343307852745\n",
      "step: 90, loss: 0.0003537096781656146\n",
      "step: 100, loss: 0.004732795059680939\n",
      "step: 110, loss: 0.0003118740860372782\n",
      "step: 120, loss: 0.03000607341527939\n",
      "step: 130, loss: 0.0022472585551440716\n",
      "step: 140, loss: 0.0026460078079253435\n",
      "step: 150, loss: 0.0002641431929077953\n",
      "step: 160, loss: 0.0056867701932787895\n",
      "step: 170, loss: 0.0006883419118821621\n",
      "step: 180, loss: 0.018692830577492714\n",
      "step: 190, loss: 0.0006843097507953644\n",
      "step: 200, loss: 0.00288579473271966\n",
      "step: 210, loss: 0.006719072815030813\n",
      "step: 220, loss: 0.0006093308911658823\n",
      "step: 230, loss: 0.0004791855171788484\n",
      "step: 240, loss: 0.033048130571842194\n",
      "step: 250, loss: 0.0021206564269959927\n",
      "step: 260, loss: 0.0021991822868585587\n",
      "step: 270, loss: 0.0023398420307785273\n",
      "step: 280, loss: 0.0006134662544354796\n",
      "step: 290, loss: 0.0010719751007854939\n",
      "step: 300, loss: 0.011812010779976845\n",
      "step: 310, loss: 0.04132862016558647\n",
      "step: 320, loss: 0.0007072054431773722\n",
      "step: 330, loss: 0.02308838441967964\n",
      "step: 340, loss: 0.014578164555132389\n",
      "step: 350, loss: 0.01323730032891035\n",
      "step: 360, loss: 0.0467422753572464\n",
      "step: 370, loss: 0.00101819250266999\n",
      "=========eval at epoch=24=========\n",
      "num_proposed:8582\n",
      "num_correct:7886\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/24.pt\n",
      "=====sanity check======\n",
      "words: [CLS] for , goals against , points ) : [SEP]\n",
      "x: [ 101 1111  117 2513 1222  117 1827  114  131  102]\n",
      "tokens: ['[CLS]', 'for', ',', 'goals', 'against', ',', 'points', ')', ':', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 1 1 1 1 1 0]\n",
      "tags: <PAD> O O O O O O O O <PAD>\n",
      "seqlen: 10\n",
      "=======================\n",
      "step: 0, loss: 0.020819256082177162\n",
      "=========eval at epoch=25=========\n",
      "num_proposed:8565\n",
      "num_correct:7880\n",
      "num_gold:8603\n",
      "precision=0.92\n",
      "recall=0.92\n",
      "f1=0.92\n",
      "weights were saved to checkpoints/01/25.pt\n",
      "=====sanity check======\n",
      "words: [CLS] JAKARTA 1996-08-27 [SEP]\n",
      "x: [  101   147  1592 18135 10460  1592  1820   118  4775   118  1765   102]\n",
      "tokens: ['[CLS]', 'J', '##A', '##KA', '##RT', '##A', '1996', '-', '08', '-', '27', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "y: [0 8 0 0 0 0 1 0 0 0 0 0]\n",
      "tags: <PAD> B-LOC O <PAD>\n",
      "seqlen: 12\n",
      "=======================\n",
      "step: 0, loss: 0.001210376969538629\n",
      "step: 10, loss: 0.0006970917456783354\n",
      "step: 20, loss: 0.0021851735655218363\n",
      "step: 30, loss: 0.0003047929494641721\n",
      "step: 40, loss: 0.020661570131778717\n",
      "step: 50, loss: 0.000620236387476325\n",
      "step: 60, loss: 0.0004830049874726683\n",
      "step: 70, loss: 0.01707773655653\n",
      "step: 80, loss: 0.007395019754767418\n",
      "=========eval at epoch=26=========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c6fbed062d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{fname}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c6fbed062d41>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, iterator, f)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# y_hat: (N, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_device_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 raise RuntimeError(\"module must have its parameters and buffers \"\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \"\"\"\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "# from model import Net\n",
    "# from data_load import NerDataset, pad, VOCAB, tokenizer, tag2idx, idx2tag\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i==0:\n",
    "            print(\"=====sanity check======\")\n",
    "            print(\"words:\", words[0])\n",
    "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
    "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
    "            print(\"is_heads:\", is_heads[0])\n",
    "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
    "            print(\"tags:\", tags[0])\n",
    "            print(\"seqlen:\", seqlens[0])\n",
    "            print(\"=======================\")\n",
    "\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(f\"step: {i}, loss: {loss.item()}\")\n",
    "\n",
    "def eval(model, iterator, f):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(\"temp\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(f\"{w} {t} {p}\\n\")\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    ## calc metric\n",
    "    y_true =  np.array([tag2idx[line.split()[1]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    num_proposed = len(y_pred[y_pred>1])\n",
    "    num_correct = (np.logical_and(y_true==y_pred, y_true>1)).astype(np.int).sum()\n",
    "    num_gold = len(y_true[y_true>1])\n",
    "\n",
    "    print(f\"num_proposed:{num_proposed}\")\n",
    "    print(f\"num_correct:{num_correct}\")\n",
    "    print(f\"num_gold:{num_gold}\")\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "\n",
    "    final = f + \".P%.2f_R%.2f_F%.2f\" %(precision, recall, f1)\n",
    "    with open(final, 'w') as fout:\n",
    "        result = open(\"temp\", \"r\").read()\n",
    "        fout.write(f\"{result}\\n\")\n",
    "\n",
    "        fout.write(f\"precision={precision}\\n\")\n",
    "        fout.write(f\"recall={recall}\\n\")\n",
    "        fout.write(f\"f1={f1}\\n\")\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    print(\"precision=%.2f\"%precision)\n",
    "    print(\"recall=%.2f\"%recall)\n",
    "    print(\"f1=%.2f\"%f1)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# if __name__==\"__main__\":\n",
    "if True:\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "#     parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "#     parser.add_argument(\"--n_epochs\", type=int, default=30)\n",
    "#     parser.add_argument(\"--finetuning\", dest=\"finetuning\", action=\"store_true\")\n",
    "#     parser.add_argument(\"--top_rnns\", dest=\"top_rnns\", action=\"store_true\")\n",
    "#     parser.add_argument(\"--logdir\", type=str, default=\"checkpoints/01\")\n",
    "#     parser.add_argument(\"--trainset\", type=str, default=\"conll2003/train.txt\")\n",
    "#     parser.add_argument(\"--validset\", type=str, default=\"conll2003/valid.txt\")\n",
    "#     hp = parser.parse_args()\n",
    "    torch.cuda.reset_max_memory_allocated(device=None)\n",
    "    print(torch.cuda.max_memory_allocated(device=None))\n",
    "    from inspect import currentframe, getsourcelines, getframeinfo\n",
    "    frameinfo = getframeinfo(currentframe())\n",
    "    framelines = getsourcelines(currentframe())\n",
    "    [print(line[:-1]) for line in framelines[0][frameinfo.lineno+2:frameinfo.lineno+7]]\n",
    "    batch_size = 16\n",
    "    lr = 0.0001\n",
    "    n_epochs = 30\n",
    "    finetuning = True\n",
    "    top_rnns = False\n",
    "    logdir = \"checkpoints/01\"\n",
    "    trainset = \"conll2003/train.txt\"\n",
    "    validset = \"conll2003/valid.txt\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     device = 'cpu'\n",
    "\n",
    "#     model = Net(hp.top_rnns, len(VOCAB), device, hp.finetuning).cuda()\n",
    "#     model = Net(top_rnns=top_rnns, vocab_size=len(VOCAB), device=device, finetuning=finetuning).cuda()\n",
    "    model = Net(top_rnns=top_rnns, vocab_size=len(VOCAB), device=device, finetuning=finetuning)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(torch.cuda.max_memory_allocated(device=None))\n",
    "    train_dataset = NerDataset(trainset)\n",
    "    eval_dataset = NerDataset(validset)\n",
    "\n",
    "    train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=4,\n",
    "                                 collate_fn=pad)\n",
    "    eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=4,\n",
    "                                 collate_fn=pad)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    print(len(train_iter))\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        try:\n",
    "            train(model, train_iter, optimizer, criterion)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"=========eval at epoch={epoch}=========\")\n",
    "        if not os.path.exists(logdir): os.makedirs(logdir)\n",
    "        fname = os.path.join(logdir, str(epoch))\n",
    "        precision, recall, f1 = eval(model, eval_iter, fname)\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{fname}.pt\")\n",
    "        print(f\"weights were saved to {fname}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LBzKpWFC6nR2",
    "outputId": "3bb42732-878c-430a-dbcb-864e034843a1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNgs80KECQxw"
   },
   "source": [
    "if cuda torch fails https://pytorch.org/get-started/locally/#cuda-100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRTHK6nkCVPI"
   },
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_max_memory_allocated(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "    \"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "    assert(isinstance(size, torch.Size))\n",
    "    return \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "    import gc\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "                                        \" GPU\" if obj.is_cuda else \"\",\n",
    "                                        \" pinned\" if obj.is_pinned else \"\",\n",
    "                                        pretty_size(obj.size())))\n",
    "                    total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "                                                   type(obj.data).__name__, \n",
    "                                                   \" GPU\" if obj.is_cuda else \"\",\n",
    "                                                   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "                                                   \" grad\" if obj.requires_grad else \"\", \n",
    "                                                   \" volatile\" if obj.volatile else \"\",\n",
    "                                                   pretty_size(obj.data.size())))\n",
    "                    total_size += obj.data.numel()\n",
    "                     \n",
    "        except Exception as e:\n",
    "            pass        \n",
    "    print(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import currentframe, getframeinfo, getsourcelines\n",
    "import traceback\n",
    "frameinfo = getframeinfo(currentframe())\n",
    "framelines = getsourcelines(currentframe())\n",
    "print (frameinfo.filename, frameinfo.lineno)\n",
    "[print(line) for line in framelines[0][frameinfo.lineno-2:frameinfo.lineno]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ner_conll_2003.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
